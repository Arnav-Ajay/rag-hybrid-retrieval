question_id,question_text,question_intent,gold_chunk_id,gold_doc_id,retrieved_chunk_ids_dense,rank_of_first_relevant_dense,retrieved_in_top_k_dense,notes,retrieved_chunk_ids_sparse,rank_of_first_relevant_sparse,retrieved_in_top_k_sparse,retrieved_chunk_ids_hybrid,rank_of_first_relevant_hybrid,retrieved_in_top_k_hybrid,delta_rank
1,What architectural components are removed in the Transformer model?,definition,1,1,56|630|276|130|232|61|609|618|536|54|178|17|488|565|627|491|291|1|364|83,18,False,,386|539|111|290|389|212|302|170|229|105|502|385|155|192|253|214|288|188|109|198,,False,386|539|111|290|389|212|302|170|229|105|502|385|155|192|253|214|288|188|109|198,,False,
2,What aspect of recurrent neural networks prevents parallelization during training?,mechanism,6,1,630|536|61|56|130|54|276|17|232|565|62|581|1|291|84|49|518|15|572|123,,False,,74|408|56|410|152|82|76|83|230|247|302|539|607|70|231|239|197|192|36|283,,False,56|74|408|410|152|82|76|83|230|247|302|539|607|70|231|239|197|192|36|283,,False,
3,What capability do attention mechanisms provide for modeling dependencies in sequences?,mechanism,7,1,630|130|56|536|61|17|54|276|232|581|15|627|62|495|109|386|49|565|31|24,,False,,386|86|24|9|233|176|200|105|89|373|171|5|56|257|6|362|540|172|166|268,,False,56|386|24|86|9|233|176|200|105|89|373|171|5|257|6|362|540|172|166|268,,False,
4,What mechanism does the Transformer rely on instead of recurrence?,definition,8,1,56|630|130|276|232|178|627|61|565|609|491|618|17|580|536|54|291|561|20|83,,False,,171|1|542|510|128|390|387|302|190|111|191|207|104|539|253|348|109|105|192|210,,False,171|1|542|510|128|390|387|302|190|111|191|207|104|539|253|348|109|105|192|210,,False,
5,How does the number of operations required to relate distant positions differ between convolutional models and the Transformer?,comparison,9,1,41|8|591|586|573|538|9|572|31|529|26|588|38|517|4|53|300|6|612|562,7,False,,9|245|204|52|357|155|200|29|240|226|237|496|205|34|16|310|242|246|87|278,1,True,9|245|204|52|357|155|200|29|240|226|237|496|205|34|16|310|242|246|87|278,1,True,6
6,How is self-attention defined?,definition,10,1,595|375|628|627|131|171|568|241|178|220|395|630|565|616|228|195|636|274|545|299,,False,,10|200|206|260|29|105|259|32|386|24|164|173|86|174|205|350|201|168|171|310,1,True,171|10|200|206|260|29|105|259|32|386|24|164|173|86|174|205|350|201|168|310,2,True,
7,What distinguishes the Transformer from earlier transduction models?,definition,11,1,56|630|276|232|130|536|61|491|17|178|609|627|54|1|83|565|229|618|591|571,,False,,11|248|111|302|539|228|150|348|331|496|155|379|386|1|170|169|214|253|127|182,1,True,1|11|248|111|302|539|228|150|348|331|496|155|379|386|170|169|214|253|127|182,2,True,
8,What overall encoder–decoder architecture does the Transformer use?,definition,13,1,605|431|384|491|523|54|522|56|124|354|130|171|509|627|432|393|397|67|84|617,,False,,1|154|183|161|90|173|226|24|111|120|170|187|302|231|94|150|386|119|181|169,,False,1|154|183|161|90|173|226|24|111|120|170|187|302|231|94|150|386|119|181|169,,False,
9,What two sub-layers make up each encoder layer?,structure,14,1,627|576|565|630|491|178|628|274|580|56|616|352|55|609|59|132|276|232|182|241,,False,,14|13|173|15|26|23|174|41|152|179|240|16|114|333|331|302|386|92|204|203,1,True,14|13|173|15|26|23|174|41|152|179|240|16|114|333|331|302|386|92|204|203,1,True,
10,What additional sub-layer is added to each decoder layer?,structure,15,1,491|565|56|627|630|130|232|178|276|59|1|11|17|274|182|15|555|609|48|61,16,False,,14|15|173|16|114|174|13|41|201|150|26|92|214|34|103|27|192|128|240|213,2,True,15|14|173|16|114|174|13|41|201|150|26|92|214|34|103|27|192|128|240|213,1,True,15
11,Why is masking applied in the decoder self-attention layer?,mechanism,16,1,56|565|627|491|630|232|130|276|178|1|61|536|59|11|182|488|257|17|291|83,,False,,16|174|15|173|214|24|386|34|200|114|245|10|105|14|213|29|86|32|205|90,1,True,16|174|15|173|214|24|386|34|200|114|245|10|105|14|213|29|86|32|205|90,1,True,
12,What operation is applied to scaled dot products before weighting the values?,formula,18,1,630|56|61|536|276|130|17|54|1|232|62|565|291|49|581|31|83|627|182|84,,False,,17|18|245|19|177|176|391|231|302|182|105|21|270|390|175|16|392|201|244|114,2,True,17|182|18|245|19|177|176|391|231|302|105|21|270|390|175|16|392|201|244|114,3,True,
13,What advantage does multi-head attention provide?,mechanism,21,1,491|576|627|565|630|56|580|130|628|232|274|178|276|128|17|55|182|1|241|609,,False,,10|17|174|175|4|22|173|15|51|18|206|395|210|105|386|396|631|542|390|302,,False,17|10|174|175|4|22|173|15|51|18|206|395|210|105|386|396|631|542|390|302,,False,
14,How are illegal attention connections prevented in the decoder?,mechanism,25,1,56|627|565|130|630|232|178|276|59|1|83|536|491|291|61|182|11|17|15|62,,False,,200|15|386|184|16|206|24|174|182|90|108|380|1|87|162|183|111|581|261|299,,False,1|182|15|200|386|184|16|206|24|174|90|108|380|87|162|183|111|581|261|299,,False,
15,What is the functional form of the position-wise feed-forward network?,formula,26,1,56|630|276|232|130|536|609|178|17|62|229|61|491|83|1|226|291|20|31|49,,False,,174|213|214|204|205|244|206|192|13|302|66|105|292|286|148|104|14|243|282|245,,False,174|213|214|204|205|244|206|192|13|302|66|105|292|286|148|104|14|243|282|245,,False,
16,What mathematical functions are used to construct positional encodings?,formula,30,1,56|630|276|130|232|609|61|536|17|178|62|488|49|83|1|182|291|229|20|84,,False,,212|199|244|207|211|186|192|253|198|204|301|302|220|200|88|394|157|280|393|127,,False,212|199|244|207|211|186|192|253|198|204|301|302|220|200|88|394|157|280|393|127,,False,
17,How do statistical language models compute the probability of text?,definitional,66,2,56|630|276|491|232|130|61|178|627|536|488|609|17|618|291|1|62|565|49|83,,False,,70|235|67|87|65|233|155|66|86|310|261|407|496|68|186|256|399|400|72|91,8,False,70|235|67|87|65|233|155|66|86|310|261|407|496|68|186|256|399|400|72|91,8,False,
18,What context length does an n-gram model condition on?,definitional,67,2,491|627|630|565|56|178|276|576|59|128|232|17|555|130|182|11|580|1|536|291,,False,,385|302|67|384|105|510|226|348|356|168|34|542|35|383|175|301|32|79|70|253,3,True,385|302|67|384|105|510|226|348|356|168|34|542|35|383|175|301|32|79|70|253,3,True,
19,What technique is used to mitigate data sparsity in n-gram models?,mechanism,68,2,56|630|276|130|61|565|491|609|17|232|83|536|178|1|627|618|114|364|291|580,,False,,69|70|68|228|67|192|198|337|298|237|302|510|276|152|356|190|280|253|209|279,3,True,276|69|70|68|228|67|192|198|337|298|237|302|510|152|356|190|280|253|209|279,4,True,
20,How do neural language models address data sparsity differently from n-gram models?,comparative,70,2,630|130|56|536|232|276|54|61|565|17|62|518|1|15|572|49|163|109|386|581,,False,,70|69|67|87|83|198|82|359|183|356|90|417|445|180|65|76|382|426|437|450,1,True,70|69|67|87|83|198|82|359|183|356|90|417|445|180|65|76|382|426|437|450,1,True,
21,What property of embedding vectors enables semantic similarity computation?,conceptual,71,2,56|630|276|130|178|536|232|61|17|83|62|1|627|571|609|491|84|31|565|518,,False,,71|70|91|510|351|279|278|509|512|508|447|7|6|92|48|302|501|620|239|57,1,True,71|70|91|510|351|279|278|509|512|508|447|7|6|92|48|302|501|620|239|57,1,True,
22,What distinguishes pre-trained language models from early neural language models?,conceptual,73,2,630|56|61|536|130|232|17|276|581|1|565|49|54|15|229|62|123|291|572|31,,False,,87|82|65|582|417|111|112|81|76|73|396|160|90|127|207|92|91|516|183|154,10,False,87|82|65|582|417|111|112|81|76|73|396|160|90|127|207|92|91|516|183|154,10,False,
23,What are the two stages of the pre-training and fine-tuning paradigm?,process,74,2,56|630|232|276|130|536|178|17|609|1|83|84|15|291|627|145|571|62|31|20,,False,,74|209|210|211|132|187|193|215|356|152|88|102|539|106|384|302|288|207|253|87,1,True,74|209|210|211|132|187|193|215|356|152|88|102|539|106|384|302|288|207|253|87,1,True,
24,How are large language models defined in terms of scale and architecture?,definitional,76,2,56|630|130|276|536|232|62|609|61|17|1|178|49|488|20|31|83|54|565|291,,False,,87|382|72|293|381|386|160|259|111|330|153|110|165|154|257|310|395|234|484|164,,False,87|382|72|293|381|386|160|259|111|330|153|110|165|154|257|310|395|234|484|164,,False,
25,What emergent abilities are observed in large language models?,enumeration,80,2,627|565|56|630|232|276|130|178|59|491|11|182|609|61|121|536|229|1|48|291,,False,,111|80|79|332|113|331|87|84|302|382|384|381|72|192|65|141|359|386|363|110,2,True,111|80|79|332|113|331|87|84|302|382|384|381|72|192|65|141|359|386|363|110,2,True,
26,What capability does instruction tuning enable?,capability,81,2,627|178|565|576|491|628|274|276|630|56|609|232|580|616|55|241|105|128|228|130,,False,,356|187|386|210|216|143|120|373|141|80|207|217|88|331|384|253|133|364|142|167,,False,356|187|386|210|216|143|120|373|141|80|207|217|88|331|384|253|133|364|142|167,,False,
27,Which neural architectures were commonly used before Transformers?,historical,83,2,56|630|232|276|130|536|61|291|565|178|488|84|609|627|59|491|17|54|1|83,20,False,,387|182|90|207|495|232|154|313|516|271|270|245|205|131|230|76|340|88|383|153,,False,232|387|182|90|207|495|154|313|516|271|270|245|205|131|230|76|340|88|383|153,,False,
28,What advantage do Transformers have over recurrent neural networks?,comparative,85,2,56|630|130|232|276|61|17|536|54|618|178|609|364|491|565|291|561|1|627|488,,False,,408|56|74|410|257|86|140|155|82|76|83|152|607|228|65|181|70|229|384|253,,False,56|408|74|410|257|86|140|155|82|76|83|152|607|228|65|181|70|229|384|253,,False,
29,What components make up the BERT architecture?,structural,91,2,627|576|565|178|630|274|628|491|55|559|616|56|276|241|609|228|232|204|224|49,,False,,386|539|302|154|288|203|253|387|152|395|100|101|229|92|307|533|231|102|150|290,,False,386|539|302|154|288|203|253|387|152|395|100|101|229|92|307|533|231|102|150|290,,False,
30,What pre-training objectives are used in BERT?,enumeration,92,2,627|628|576|565|630|55|616|274|491|178|580|228|56|352|432|204|241|232|276|59,,False,,92|152|164|184|192|106|539|209|302|102|111|210|179|253|207|88|242|100|73|393,1,True,92|152|164|184|192|106|539|209|302|102|111|210|179|253|207|88|242|100|73|393,1,True,
31,How does replaced token detection differ from masked language modeling?,mechanism,104,2,56|630|276|232|130|536|178|609|61|62|488|17|83|182|1|572|84|54|20|616,,False,,106|200|211|187|92|70|120|87|71|226|198|222|459|206|228|213|201|580|150|299,,False,106|200|211|187|92|70|120|87|71|226|198|222|459|206|228|213|201|580|150|299,,False,
32,What training approaches are used in XLM?,enumeration,107,2,627|628|491|630|565|375|616|540|228|241|274|131|55|178|395|576|479|195|204|182,,False,,210|235|192|302|253|179|220|211|380|539|249|394|111|386|152|218|313|209|88|153,,False,210|235|192|302|253|179|220|211|380|539|249|394|111|386|152|218|313|209|88|153,,False,
33,What architectural modifications distinguish LLaMA from GPT-3?,enumeration,128,2,56|565|627|630|491|276|232|178|130|11|59|536|61|182|17|1|229|576|291|609,,False,,111|114|127|96|128|386|122|112|228|373|136|356|95|137|365|67|213|90|126|117,5,False,111|114|127|96|128|386|122|112|228|373|136|356|95|137|365|67|213|90|126|117,5,False,
34,What scaling rule does the Chinchilla study identify for compute-optimal training?,principle,155,2,630|536|56|130|61|232|15|276|17|54|1|49|565|386|84|572|581|123|109|291,,False,,155|156|399|166|190|235|242|539|302|220|203|210|192|542|254|419|66|243|421|226,1,True,155|156|399|166|190|235|242|539|302|220|203|210|192|542|254|419|66|243|421|226,1,True,
35,What types of memory are combined in retrieval-augmented generation models?,definition,516,3,56|630|276|61|130|536|232|178|17|263|83|609|54|627|291|1|565|561|49|182,,False,,293|292|192|288|465|302|214|265|253|111|514|261|79|467|384|463|528|395|299|386,,False,293|292|192|288|465|302|214|265|253|111|514|261|79|467|384|463|528|395|299|386,,False,
36,What serves as the non-parametric memory in RAG models?,definition,516,3,491|627|565|56|630|276|312|128|59|232|291|536|576|61|130|11|580|17|274|47,,False,,526|289|546|519|586|528|231|384|253|335|111|77|302|105|291|533|386|588|532|539,,False,291|526|289|546|519|586|528|231|384|253|335|111|77|302|105|533|386|588|532|539,,False,
37,How do the two RAG formulations differ in how retrieved passages are used?,comparison,517,3,56|630|276|130|61|232|178|536|62|83|17|1|54|627|609|84|565|572|182|31,,False,,516|261|548|88|535|87|299|311|264|384|240|200|386|310|164|216|180|186|314|577,,False,516|261|548|88|535|87|299|311|264|384|240|200|386|310|164|216|180|186|314|577,,False,
38,What limitations of purely parametric language models are identified?,enumeration,518,3,56|630|130|276|232|609|17|178|536|491|571|61|1|15|509|627|84|580|20|291,,False,,253|111|302|288|67|254|526|386|258|87|384|399|539|586|88|382|573|114|183|519,,False,253|111|302|288|67|254|526|386|258|87|384|399|539|586|88|382|573|114|183|519,,False,
39,Which models combine masked language models with a differentiable retriever?,factual,519,3,56|630|276|536|61|130|232|178|17|609|291|627|62|1|84|54|15|182|565|581,,False,,120|152|526|516|200|178|153|92|134|381|539|113|106|131|359|65|119|87|110|154,,False,178|120|152|526|516|200|153|92|134|381|539|113|106|131|359|65|119|87|110|154,,False,
40,What search method is used to retrieve top-K documents in RAG?,mechanism,525,3,56|491|627|565|630|276|178|232|59|11|130|536|1|618|182|15|17|291|609|571,,False,,535|232|316|233|228|296|223|578|188|582|289|510|149|541|72|291|294|229|235|512,,False,232|291|535|316|233|228|296|223|578|188|582|289|510|149|541|72|294|229|235|512,,False,
41,Which sequence-to-sequence model is used as the generator in RAG?,definition,526,3,56|630|565|627|232|61|276|130|536|491|54|571|618|178|291|182|59|17|1|264,,False,,525|105|201|541|116|198|533|222|578|121|24|526|226|245|232|178|575|203|119|576,12,False,232|178|525|105|201|541|116|198|533|222|578|121|24|526|226|245|575|203|119|576,14,False,
42,What approximation is used to marginalize over retrieved documents during generation?,mechanism,527,3,630|56|130|536|61|232|17|276|54|62|15|565|109|49|581|1|572|518|386|627,,False,,577|535|152|192|242|207|210|232|71|498|384|563|553|302|392|105|582|253|340|155,,False,232|577|535|152|192|242|207|210|71|498|384|563|553|302|392|105|582|253|340|155,,False,
43,How does this approach differ from training non-parametric memory from scratch?,comparison,528,3,630|56|536|130|61|54|276|193|149|232|291|17|1|565|62|84|15|591|572|257,,False,,299|248|526|586|309|231|317|546|528|519|166|328|314|226|218|496|449|171|188|167,9,False,299|248|526|586|309|231|317|546|528|519|166|328|314|226|218|496|449|171|188|167,9,False,
44,Which open-domain QA datasets achieve state-of-the-art results with RAG?,factual,529,3,56|630|276|130|609|178|61|232|536|237|17|84|618|627|1|20|565|182|555|34,,False,,555|138|560|139|557|366|134|77|565|107|159|147|297|161|144|270|532|545|100|146,,False,565|555|138|560|139|557|366|134|77|107|159|147|297|161|144|270|532|545|100|146,,False,
45,What components make up a retrieval-augmented generation model?,definition,531,3,56|565|630|276|627|178|232|130|59|609|1|491|61|536|182|561|11|291|17|83,,False,,292|288|290|302|214|265|514|263|293|539|465|203|467|463|79|386|299|580|253|111,,False,292|288|290|302|214|265|514|263|293|539|465|203|467|463|79|386|299|580|253|111,,False,
46,What assumption does the RAG-Sequence model make about retrieved documents?,mechanism,533,3,56|630|276|130|232|178|536|61|17|1|609|15|571|627|48|62|54|291|31|565,,False,,535|577|578|202|105|541|226|384|302|533|575|203|510|197|534|576|526|297|548|178,10,False,178|535|577|578|202|105|541|226|384|302|533|575|203|510|197|534|576|526|297|548,11,False,
47,What capability does RAG-Token introduce compared to RAG-Sequence?,comparison,535,3,56|630|276|232|130|178|609|565|491|580|61|627|17|536|618|291|1|561|488|62,,False,,533|541|575|578|576|559|198|105|534|526|527|226|535|568|579|532|291|263|567|516,13,False,291|533|541|575|578|576|559|198|105|534|526|527|226|535|568|579|532|263|567|516,14,False,
48,How can RAG be adapted for sequence classification?,procedure,536,3,491|565|576|56|627|276|630|232|178|128|130|580|11|182|274|257|114|59|536|291,19,False,,541|261|341|180|311|578|164|87|565|533|92|290|116|240|289|501|575|576|314|534,,False,565|576|541|261|341|180|311|578|164|87|533|92|290|116|240|289|501|575|314|534,,False,
49,What similarity operation is used to score query–document pairs in DPR?,mechanism,537,3,222|43|491|424|575|229|345|83|471|524|123|399|232|523|136|82|77|630|276|237,,False,,510|208|175|86|348|351|192|172|176|103|105|296|302|349|295|345|563|319|275|289,,False,345|510|208|175|86|348|351|192|172|176|103|105|296|302|349|295|563|319|275|289,,False,
50,On which datasets was the DPR retriever originally trained?,factual,538,3,56|565|627|491|630|276|232|11|83|130|1|182|61|59|20|536|49|178|17|609,,False,,516|253|526|536|138|297|192|92|170|76|539|148|118|221|131|358|152|213|171|332,,False,536|516|253|526|138|297|192|92|170|76|539|148|118|221|131|358|152|213|171|332,,False,
51,What model is used as the generator in the RAG framework?,definition,539,3,491|627|565|630|56|276|232|182|130|368|1|178|83|257|11|59|58|241|121|17,,False,,539|105|384|198|313|192|104|245|228|351|348|170|296|290|393|316|302|289|201|207,1,True,539|105|384|198|313|192|104|245|228|351|348|170|296|290|393|316|302|289|201|207,1,True,
52,"Which components are fine-tuned during training, and which are kept fixed?",procedure,540,3,56|630|276|83|130|61|536|232|178|609|488|62|627|17|54|1|84|571|565|291,,False,,210|207|364|357|230|221|215|212|153|205|87|209|358|140|247|384|106|235|117|203,,False,210|207|364|357|230|221|215|212|153|205|87|209|358|140|247|384|106|235|117|203,,False,
53,What distinguishes Fast Decoding from Thorough Decoding in RAG?,comparison,543,3,56|627|565|630|232|276|491|178|130|536|59|61|17|11|1|609|182|291|48|263,,False,,541|221|564|188|88|222|540|100|416|223|617|101|480|168|289|384|348|331|228|578,,False,541|221|564|188|88|222|540|100|416|223|617|101|480|168|289|384|348|331|228|578,,False,
54,How can RAG models update world knowledge without retraining?,mechanism,573,3,565|56|627|276|630|491|83|232|130|59|61|536|1|62|182|178|291|241|11|257,,False,,263|311|251|87|290|300|88|289|359|288|533|264|81|384|287|242|249|164|541|366,,False,263|311|251|87|290|300|88|289|359|288|533|264|81|384|287|242|249|164|541|366,,False,
